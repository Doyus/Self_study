1. response 是一个‘scrapy.http.response.html.HtmlResponse对象’, 可以执行‘xpath’和css通用语法来替换数据

2. 提取出来的数据是一个'selector'或者是一个'selectorList'对象，如果想要获取其中的字符串。那么应该执行'getall'或者'get'方法

3. getall方法：获取selector所有文本，返回的是一个列表

4. get()方法，获取的是‘selector’中的第一个文本，返回的是一个str类型

5. 如果数据解析回来，要传给pipline，那么可以使用yield来返回，或者是收集所有的item，最后统一return返回

6. item： 建议在items.py 中定义好模型，以后就不要使用字典

7. pipeline: 这个是专门用来保存数据的，其中有三个方法是会常用的。
    1）open_spider(self,spider):当爬虫被打开的时候调用
    2）process_item(self,item,spider):当爬虫有item传过来的时候回调用
    3） close_spider(self,spider):当爬虫关闭的时候被调用

    要激活pipline,应该在setting.py中设置ITEM_PIPELTNES

## JsonItemExporter He  JsonLinesItemExporter:
保存数据的时候，可以使用这两个类，让操作变得简单。
1. JsonItemExporterL: 这个是每次把数据添加到内存中，最后统一写入磁盘中，好处是，存储的数据是一个满足json的数据，坏处是如果数据量比较大，那么比较耗内存

2. JsonLinesItemExporter： 这个是每次调用export_item的时候就把这个item储存到硬盘中，坏处是每一个字典是一行，整个文件不是一个满足json格式的文件，好处就是就是不太耗内存，数据也比较安全。


